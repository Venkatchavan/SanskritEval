# SanskritEval ğŸ•‰ï¸

**Probing Sandhi and Case Generalization in Language Models**

A benchmark suite for evaluating how well language models handle Sanskrit-specific linguistic phenomena, specifically sandhi (phonological fusion at word boundaries) and morphological case agreement.

## ğŸ¯ Project Goals

Sanskrit represents a critical test case for evaluating language model capabilities on:
- **Structurally different low-resource languages**
- **Complex morphological patterns** (8 cases Ã— 3 numbers Ã— 3 genders)
- **Abstract linguistic rules** (sandhi transformations at phonological boundaries)

This benchmark probes whether LMs learn genuine abstraction mechanisms or merely surface-level patterns.

## ğŸ“Š Tasks

### Task A: Sandhi Segmentation
**Goal**: Detect word boundaries in phonologically fused Sanskrit strings.

- **Input**: Fused sandhi form (e.g., `rÄmo'gacchat`)
- **Output**: Segmented form with boundaries (e.g., `rÄmaá¸¥ agacchat`)
- **Metrics**: Precision, Recall, F1 on boundary detection

### Task B: Morphological Acceptability (Contrast Sets)
**Goal**: Test if models have learned case/agreement patterns.

- **Input**: Minimal pairs (grammatical vs. ungrammatical)
  - âœ“ Grammatical: correct case ending/agreement
  - âœ— Ungrammatical: incorrect case ending/agreement
- **Scoring**: Log-likelihood or perplexity comparison
- **Metrics**: Accuracy (% pairs where LM prefers grammatical variant)

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/Venkatchavan/SanskritEval.git
cd SanskritEval

# Create environment
conda env create -f environment.yml
conda activate sanskriteval

# Or use pip
pip install -r requirements.txt
```

### Running the Benchmark

```bash
# Generate benchmark datasets
make generate-data

# Run evaluation on all models
make evaluate

# Generate report and plots
make report
```

## ğŸ“ Project Structure

```
sanskriteval/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw Sanskrit corpora
â”‚   â”œâ”€â”€ processed/        # Cleaned and preprocessed data
â”‚   â””â”€â”€ benchmarks/       # Final benchmark datasets (JSONL/CSV)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ sanskriteval/     # Main package
â”‚       â”œâ”€â”€ data/         # Data generation and processing
â”‚       â”œâ”€â”€ models/       # Model wrappers and evaluation
â”‚       â”œâ”€â”€ metrics/      # Evaluation metrics
â”‚       â””â”€â”€ utils/        # Utilities
â”œâ”€â”€ scripts/              # Standalone scripts for data/eval
â”œâ”€â”€ notebooks/            # Jupyter notebooks for exploration
â”œâ”€â”€ reports/              # Generated reports and plots
â”œâ”€â”€ Makefile              # Automation commands
â””â”€â”€ README.md
```

## ğŸ“¦ Deliverables

- âœ… **Benchmark Dataset**: JSONL/CSV format with generation scripts
- âœ… **Evaluation Pipeline**: Reproducible model evaluation framework
- âœ… **Results**: Models Ã— Tasks comparison table + visualizations
- âœ… **Report**: 6-8 page paper-style documentation
- ğŸ¯ **Optional**: Zenodo DOI for dataset release

## ğŸ”¬ Evaluated Models

- GPT-3.5/GPT-4 (OpenAI)
- Claude (Anthropic)
- Gemini (Google)
- LLaMA variants
- Multilingual models (mBERT, XLM-R)
- Sanskrit-specific models (if available)

## ğŸ“š Citation

If you use this benchmark, please cite:

```bibtex
@misc{sanskriteval2025,
  title={SanskritEval: Probing Sandhi and Case Generalization in Language Models},
  author={[Your Name]},
  year={2025},
  url={https://github.com/Venkatchavan/SanskritEval}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

- GitHub: [@Venkatchavan](https://github.com/Venkatchavan)
- Issues: [GitHub Issues](https://github.com/Venkatchavan/SanskritEval/issues)

---

**Note**: This is an active research project. Dataset and evaluation scripts will be continuously updated.
