# SanskritEval ğŸ•‰ï¸

**Probing Sandhi and Case Generalization in Language Models**

A benchmark suite for evaluating how well language models handle Sanskrit-specific linguistic phenomena, specifically sandhi (phonological fusion at word boundaries) and morphological case agreement.

## ğŸ¯ Project Goals

Sanskrit represents a critical test case for evaluating language model capabilities on:
- **Structurally different low-resource languages**
- **Complex morphological patterns** (8 cases Ã— 3 numbers Ã— 3 genders)
- **Abstract linguistic rules** (sandhi transformations at phonological boundaries)

This benchmark probes whether LMs learn genuine abstraction mechanisms or merely surface-level patterns.

## ğŸ“Š Tasks

### Task A: Sandhi Segmentation
**Goal**: Detect word boundaries in phonologically fused Sanskrit strings.

- **Input**: Fused sandhi form (e.g., `rÄmo'gacchat`)
- **Output**: Segmented form with boundaries (e.g., `rÄmaá¸¥ agacchat`)
- **Metrics**: Precision, Recall, F1 on boundary detection

### Task B: Morphological Acceptability (Contrast Sets)
**Goal**: Test if models have learned case/agreement patterns.

- **Input**: Minimal pairs (grammatical vs. ungrammatical)
  - âœ“ Grammatical: correct case ending/agreement
  - âœ— Ungrammatical: incorrect case ending/agreement
- **Scoring**: Log-likelihood or perplexity comparison
- **Metrics**: Accuracy (% pairs where LM prefers grammatical variant)

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/Venkatchavan/SanskritEval.git
cd SanskritEval

# Create environment
conda env create -f environment.yml
conda activate sanskriteval

# Or use pip
pip install -r requirements.txt
```

### Running the Benchmark

```bash
# Generate sandhi dataset (701 silver + 200 gold)
python scripts/generate_sandhi_data.py

# Generate morphology contrast sets (500 pairs)
python scripts/generate_morph_data.py

# Evaluate sandhi segmentation (rule-based baseline)
python scripts/evaluate_sandhi.py

# Evaluate morphology (transformer models)
python scripts/evaluate_morphology.py --models mbert xlm-r-base --sample 50

# Run complete benchmark
python scripts/run_benchmark.py --models mbert xlm-r-base indicbert

# Generate plots
python scripts/generate_plots.py
```

## ğŸ“ˆ Current Status

### âœ… Completed (Phases 0-4)

- **Data Collection**: 701 verses from Bhagavad Gita
- **Sandhi Dataset**: 701 silver training + 200 gold test examples
- **Morphology Dataset**: 500 contrast pairs (333 case + 167 number)
- **Evaluation Framework**: Metrics, model wrappers, visualization
- **Rule-Based Baseline**: 1.000 F1 on sandhi segmentation

### ğŸ“Š Dataset Statistics

| Dataset | Examples | Description |
|---------|----------|-------------|
| Sandhi Silver Train | 701 | Rule-based segmentations |
| Sandhi Gold Test | 200 | Stratified samples (needs manual annotation) |
| Morphology Contrast Sets | 500 | Minimal pairs (case/number perturbations) |
| Source Corpus | 701 verses | Bhagavad Gita |

### ğŸ¯ Models Ready to Evaluate

- **mBERT**: `bert-base-multilingual-cased` (110M params)
- **XLM-R Base**: `xlm-roberta-base` (270M params)
- **XLM-R Large**: `xlm-roberta-large` (550M params)
- **IndicBERT**: `ai4bharat/indic-bert` (110M, Indic languages)
- **MuRIL**: `google/muril-base-cased` (235M, Indian languages)

## ğŸ“ Project Structure

```
sanskriteval/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # 701 verses from Bhagavad Gita
â”‚   â””â”€â”€ benchmarks/             # Sandhi (701+200) + Morphology (500)
â”œâ”€â”€ src/sanskriteval/
â”‚   â”œâ”€â”€ data/                   # Data generation (normalizer, sandhi, morphology)
â”‚   â”œâ”€â”€ models/                 # Model wrappers (rule-based, transformers)
â”‚   â”œâ”€â”€ metrics/                # Evaluation metrics (sandhi, morphology)
â”‚   â””â”€â”€ utils/                  # Text processing, config, logging
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_*.py           # Dataset generation
â”‚   â”œâ”€â”€ evaluate_*.py           # Task evaluation
â”‚   â”œâ”€â”€ run_benchmark.py        # Unified benchmark runner
â”‚   â””â”€â”€ generate_plots.py       # Results visualization
â”œâ”€â”€ docs/                       # Phase documentation
â”œâ”€â”€ results/                    # Evaluation outputs (JSON, CSV, plots)
â””â”€â”€ README.md
```

## ğŸ“¦ Deliverables

- âœ… **Benchmark Dataset**: 
  - Sandhi: 701 training + 200 test (JSONL)
  - Morphology: 500 contrast pairs (JSONL)
  - Generation scripts included
- âœ… **Evaluation Pipeline**: 
  - Metrics: P/R/F1 for sandhi, acceptability accuracy for morphology
  - Model wrappers: Rule-based + 5 transformer models
  - Unified runner with CSV summary
- âœ… **Visualization**: 4 plot types (sandhi, morphology overall/breakdown, combined)
- âš ï¸ **Results**: Models Ã— Tasks comparison (in progress)
- âš ï¸ **Report**: 6-8 page paper-style documentation (Phase 5)

## ğŸ”¬ Baseline Results

### Sandhi Segmentation (Rule-Based)

| Model | Precision | Recall | F1 | Exact Match |
|-------|-----------|--------|-------|-------------|
| Rule-Based | 1.000 | 1.000 | 1.000 | 1.000 |

*Note*: Gold set was generated using same splitter; manual annotation needed for real evaluation.

### Morphological Acceptability (Expected)

| Model | Expected Accuracy | Notes |
|-------|------------------|-------|
| Random | 50% | Baseline |
| mBERT | 60-70% | Multilingual |
| XLM-R Base | 70-80% | Stronger multilingual |
| IndicBERT | 75-85% | Indic-specialized |

## ğŸ“š Citation

If you use this benchmark, please cite:

```bibtex
@misc{sanskriteval2025,
  title={SanskritEval: Probing Sandhi and Case Generalization in Language Models},
  author={[Your Name]},
  year={2025},
  url={https://github.com/Venkatchavan/SanskritEval}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

- GitHub: [@Venkatchavan](https://github.com/Venkatchavan)
- Issues: [GitHub Issues](https://github.com/Venkatchavan/SanskritEval/issues)

---

**Note**: This is an active research project. Dataset and evaluation scripts will be continuously updated.
